{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <h1>Welcome to Intro to Data Analytics</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data analytics](https://ofdataanalytics.com/wp-content/uploads/2019/08/Screenshot-2019-08-22-at-22.15.17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have probably heard of, and maybe even researched on your own, terms such as data analytics, data sciece, machine learning, deep learning, artificial intelligence, data mining, the list goes on, and wonder what's the big fuzz about these new terms and/or how does one get started in roles involving these (forgive the buzz) words. Whale 🐳, let's cover these on a high level and move on to the very beginnig, or the core skill one would need in order to jump into roles with these terms, __data analysis__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"This is my favorite part about analytics: Taking boring flat data and bringing it to life through visualization.\" ~ John Tukey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline 4 Today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is Data Analytics, what is a data analyst, and what does the Data Analytics Cycle?\n",
    "2. What is Python and why is it so popular?\n",
    "3. Coming up with an idea or project\n",
    "4. Getting the data we'll need\n",
    "5. Preparing the data for analysis\n",
    "6. Analysing and visualising data\n",
    "7. Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Data Analytics, what is a data analyst, and what does the Data Analytics Cycle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data analytics](https://i2.wp.com/www.upbuild.io/wp-content/uploads/2018/07/upbuild-what-are-your-analytics-telling-you.png?w=1364&ssl=1)\n",
    "- __Data Analytics__\n",
    "\n",
    "Data analytics is a process for uncovering insights from information. This information can structured or unstructured, qualitative or quantitative, time-dependent or -independent, and highly-dimensional or with only one dimension.\n",
    "\n",
    "You can also think of data analytics as the following:\n",
    "\n",
    "    - A process for finding answers to questions\n",
    "    - Interrogation of the facts and evaluation of the future\n",
    "    - Information decomposition\n",
    "    - Automagic sometimes\n",
    "    - An evolving framework for the inquisitive mind\n",
    "    \n",
    "![detective](https://media.giphy.com/media/NS7gPxeumewkWDOIxi/giphy.gif)\n",
    "- __Data Analyst__\n",
    "\n",
    "A data analyst can be thought of as a detective. Detectives look at the evidence of a case, analyse it and ask many (many) questions throughout the process, make up inferences based on the evidence, narrow down a conclusion when everything is said and done. This is, in a way, the same process a data analyst undertakes for a project. To understand this better, let's look at the Data Analytics Cycle.\n",
    "\n",
    "- __Data Analytics Cycle__\n",
    "\n",
    "The data analytics cycle is an iterative process that data analysts go through in their day-to-day activities. This may vary depending on the type of company and responsabilities of the analyst, but for the most part, we can look at the cycle (from a high level) in the following way.\n",
    "\n",
    "    1. Define the problem or question to be solved\n",
    "    2. What can of data will we need to solve this and where do we get it from?\n",
    "    3. Cleaning or structuring the data in a way that is suitable for analysis (very often, this may be the most time-consuming stage of the cycle)\n",
    "    4. Exploratory Data Analysis and Visualisation\n",
    "    5. Hypothesis testing or inference stage - making predictions\n",
    "    6. Reporting our findings\n",
    "    7. Presenting our findings and collecting feedback\n",
    "    8. Start again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is Python and why is it so popular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is a general-purpose programming language that allows us to create programs, analyse data, create websites, create applications, and many other cool things. It is free and open-source, which means that anyone can contribute to its development and help make this language an even better one. This latter fact, along with the great readability of the language, are (to your host) two of the major contributing factors of Python's popularity.\n",
    "\n",
    "Python can be thought of as a person, we are very cool the way we are but to interact more efficiently we make use of \"add-ons\", and thus, so does Python. These add-ons may be clothes, shoes, accessories, slang words, and other physical objects such as cars, houses, boats, etc. In Python, our add-ons become additional programs other people have created in order to make a specific workflow easier.\n",
    "\n",
    "Python in the domain of data analytics uses very heavily the following packages/libraries (and so are we today).\n",
    "\n",
    "Some of the packages/libraries  most heavily used in data analytics are:*\n",
    "\n",
    "#### For Data Analysis\n",
    "\n",
    "- [pandas](https://pandas.pydata.org/) -> \"is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\"\n",
    "\n",
    "- [numpy](https://numpy.org/) -> \"is the fundamental package for scientific computing with Python. It contains among other things, a powerful N-dimensional array object, sophisticated (broadcasting) functions, tools for integrating C/C++ and Fortran code, useful linear algebra, Fourier transform, and random number capabilities.\"\n",
    "\n",
    "- [SciPy](https://www.scipy.org/) -> \"is a Python-based ecosystem of open-source software for mathematics, science, and engineering.\"\n",
    "\n",
    "#### For Data Visualisation\n",
    "\n",
    "- [matplotlib](https://matplotlib.org/) -> \"is a comprehensive library for creating static, animated, and interactive visualizations in Python.\"\n",
    "\n",
    "- [seaborn](https://seaborn.pydata.org/) -> \"s a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\"\n",
    "\n",
    "- [altair](https://altair-viz.github.io/index.html) -> Although Altair has not had the same adoption from the community as the previous two data visualisation libraries, it is a great library for visualising your data.\n",
    "\n",
    "\\* Definitions have been taken straight from the packages respective websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coming up with an idea or project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first come up with a good scenario for our data analytics project. After all, we can't just get down to analysing data without having at least a bit of context as to what we would like to do with the data and what are we hoping to find.\n",
    "\n",
    "#### I. The Scenario\n",
    "\n",
    "Say you have always exercised at least 5 days a week, or that since last year, as your New Year's resolution, you chose to exercise for at least 5 days a week throughout the entire year. This being a completely novel endeavor for you, something no one else picks during New Year's (😎), you decide it would good to also track your progress throughout your journey, so you get a smart watch to aid you in this regard.\n",
    "\n",
    "You start noticing how most of the apps you use and websites you visit all gather a lot of data about you. Netflix captures your taste of movie for any given day and time. Facebook know which kinds of posts you like the most, and has a good sense regarding your political inclinations based on the things you post. Amazon always seems to know what to recommend you to buy (and crap, it seems to work). Now you start getting curious about all of the data your smartwatch might have about you so you decide that it is time to take control of your own data before others analyse it for you.\n",
    "\n",
    "- __Me on Week 1__\n",
    "\n",
    "![week1](https://media.giphy.com/media/13Lwn87rxZSUVi/giphy.gif)\n",
    "\n",
    "#### II. The Task\n",
    "\n",
    "Now that we have a decided to do something with our data, let's put this into a general list of tasks we'd like to take on.\n",
    "\n",
    "1. Get the data out of wherever our smartwatch is keeping it at.\n",
    "2. Prepare it for analysis.\n",
    "3. Analyse it.\n",
    "4. Think of questions/hypotheses you might want test.\n",
    "5. Write down your findings.\n",
    "6. Improve your analysis by collecting some feedback and\n",
    "7. prepare your code for future use.\n",
    "\n",
    "- __Me around Valentine's Day__\n",
    "\n",
    "![Exercise](https://media.giphy.com/media/mb8QpqfFX4CtO/giphy.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Getting the data we'll need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![get data](https://media.giphy.com/media/8qrrHSsrK9xpknGVNF/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our task somewhat defined, we can move on to gathering the data we will need for our project.\n",
    "\n",
    "Depending on which smartwatch or smartphone you have, there will be different ways to access your data. I will cover two here but you are by no means limited by these tools. In fact, I would strongly encourage you to get the data for your own tool and begin your data analytics journey by analysing your own data.\n",
    "\n",
    "- __Garmin__\n",
    "    1. Go to [__Garmin Connect__](https://connect.garmin.com/signin)\n",
    "    2. Sign in with your username and password\n",
    "    3. Go to the activities section and click on __`All Activities`__\n",
    "    4. Exporting the data gets a bit tricky here because Garmin will only export items that have already loaded, so in order to get all of your data, scroll all the way down to the very end of all of your activities.\n",
    "    5. Once you reach the last one, click on the __`Export CSV`__ button at the top right hand corner. You will see a __`Activities.csv`__ file in your downloads folder.  \n",
    "    __NOTE:__ Garmin track much more data than what you will download from Garmin Connect but the format is not as user-friendly as what you would get using these steps.\n",
    "    \n",
    "- __Apple Health__\n",
    "    - First Approach\n",
    "        1. Go to the __`Health`__ app on your IPhone.\n",
    "        2. Click on your profile at the top right-hand corner.\n",
    "        3. Scroll to the very bottom and click on __`Export All Health Data`__.\n",
    "        4. A message will pop up asking you if you really want to export your own data, click Export.\n",
    "        5. Once the app finishes preparing your data, send it to your folder, email, hardrive or app of choice.\n",
    "    - Second Approach (prefer method)\n",
    "        1. Download the app called [__Workout - CSV Exporter__](https://apps.apple.com/us/app/workout-csv-exporter/id1140433100?ls=1).\n",
    "        2. Select the kind of workouts you would like to export.\n",
    "        3. The app will also only download what it has currently loaded so make sure you scroll down all the way to the bottom to download the page before you proceed to export your data.\n",
    "        4. Click on the share icon at the top and select how you would like to export your data.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use data in Python, we first need a couple of things to help us read it into memory. Just like humans have tools to build houses, skycrapers, airplanes, and more, Python has tools for data analytics that can enhance our productivity in extraordinary ways. The python packages `pandas`, `numpy`, `matplotlib`, and `seaborn` are some of the tools that will make our work much easier today as we put on our detective hats on.\n",
    "\n",
    "Let's begin by loading these for packages first. To do this we will need to use the `import` command which tells Python that we want to bring in an object from our storage into our environment (a.k.a. this notebook we are working on 🤓)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your packages below\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import datetime\n",
    "import altair as alt\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing you will notice is that as we imported our packages into our environment, we also used an alias so that we don't have to use the full name of the package every time we need to use each one of them.\n",
    "\n",
    "The next step we need to take is to load our dataset into memory. You will find a dataset readily available in the following link\n",
    "\n",
    "> https://raw.githubusercontent.com/ramonprz01/codevelop-march-2020/master/data/activities_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data we will use a method called `.read_csv()` from our padas library and assign it to a variable. A variable is like a bucket or a net, it can hold on to an object for us and allow us to do things with it and to it. You can also think of variables as the name of a column in a spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The word df is our variable\n",
    "# The alias pd allows us to use the pandas package\n",
    "# The .read_csv() helps us read in the data\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ramonprz01/codevelop-march-2020/master/data/activities_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we want to do after loading our data is to make sure it was loaded into memory in the right way. We can do this by calling the method `.head()` on our pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Exercise prompt__ - Now you use the method `.tail()` on your data to see the first 5 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other thing we can do to inspect our dataset and make sure it was imported successfully is to look at the shape of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first have a look at our dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Cleaning / Preparation for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by having a look at the columns of our dataframe. We can accomplish this by calling the attribute `.columns` on our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the variables in any dataset can often come with spaces and some uppercase letters. I find it useful to convert these into lower case and substitute the space with an underscore, so let's do that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('--', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['avg_vertical_ratio', 'avg_vertical_oscillation',\n",
    "         'training_stress_score®', 'grit', 'flow', 'favorite',\n",
    "         'bottom_time', 'surface_interval', 'best_lap_time', \n",
    "         'max_temp', 'decompression'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['time'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['calories'] = pd.to_numeric(df['calories'])\n",
    "df['aerobic_te'] = pd.to_numeric(df['aerobic_te'])\n",
    "df['avg_run_cadence'] = pd.to_numeric(df['avg_run_cadence'])\n",
    "df['max_run_cadence'] = pd.to_numeric(df['max_run_cadence'])\n",
    "df['number_of_runs'] = pd.to_numeric(df['number_of_runs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "df['week'] = df['date'].dt.week\n",
    "df['weekday'] = df['date'].dt.weekday\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['time_exercise'] = df['date'].dt.time\n",
    "df['date_exercise'] = df['date'].dt.date\n",
    "df['day_of_week'] = df['date'].dt.weekday_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_of_day = []\n",
    "\n",
    "for i in df['time_exercise']:\n",
    "    if i > datetime.time(5, 59, 59) and i < datetime.time(12, 0, 0):\n",
    "        time_of_day.append('morning')\n",
    "    elif i > datetime.time(11, 59, 59) and i < datetime.time(18, 0, 0):\n",
    "        time_of_day.append('afternoon')\n",
    "    else:\n",
    "        time_of_day.append('night')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_of_day[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_day'] = time_of_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_or_end = []\n",
    "\n",
    "for day in df['weekday']:\n",
    "    if day >= 5:\n",
    "        week_or_end.append('weekend')\n",
    "    else:\n",
    "        week_or_end.append('week_day')\n",
    "\n",
    "week_or_end[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week_or_end'] = week_or_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Analysing and Visualising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(\n",
    "    index=['week_or_end', 'time_day'],\n",
    "    columns='year',\n",
    "    values='distance',\n",
    "    aggfunc='mean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group = df.groupby(['time_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group['activity_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group['activity_type'].value_counts(normalize=True)['night']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group['calories'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group['calories'].agg(['median', 'mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group['distance'].plot(kind='hist', title=\"Histogram of Distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['year'] == 2018, 'distance'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['distance', 'calories']].plot.hexbin(x='calories', y='distance', gridsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['distance'], bins=40, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['avg_hr'], bins=30, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='distance', y='calories', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='distance', y='calories', data=df, kind='hex');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='distance', y='calories', data=df, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['distance', 'calories', 'avg_hr', 'max_hr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df[['avg_hr', 'max_hr', 'calories', 'distance', 'title']]).mark_circle(size=60).encode(\n",
    "    x='distance',\n",
    "    y='calories',\n",
    "    color='title',\n",
    "    tooltip=['avg_hr', 'max_hr', 'calories', 'distance', 'title']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df[['avg_hr', 'max_hr', 'calories', 'distance', 'title', 'time_day']]).mark_circle(size=60).encode(\n",
    "    x='distance',\n",
    "    y='calories',\n",
    "    color='time_day',\n",
    "    tooltip=['avg_hr', 'max_hr', 'calories', 'distance', 'title', 'time_day']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df['calories'] == max(df['calories']))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = today - df[mask]['date_exercise']\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference.iloc[0].total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclusion and Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have cover a lot of things today. From data analytics to Python, from loading data and preparing it for analysis, to actually analysing and visualising our data.\n",
    "\n",
    "As your next steps you could:\n",
    "- learn python programming fundamentals\n",
    "- learn how to clean and wrangle data prior to analysing it\n",
    "- take an in-depth data analytics course and make sure you work on your own custom analyses while doing so\n",
    "- ask for feedback\n",
    "- repeat\n",
    "\n",
    "As your next, next steps you could:\n",
    "- learn to design experiments on and off-line\n",
    "- learn more about the command line and git for version control\n",
    "- create a dashboard application\n",
    "- learn data mining / machine learning\n",
    "- move into data science\n",
    "- learn a big data framework such as [Dask](https://dask.org/) or [Spark](https://spark.apache.org/)\n",
    "\n",
    "![gracias](https://media.giphy.com/media/JVXU0uN1l6wdq/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
